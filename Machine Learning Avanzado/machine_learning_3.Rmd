---
title: "Taller 3 de Machine Learning Avanzado"
author: Diabb Zegpi Delgado
output: 
  html_document:
    theme: united
    highlight: kate
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
ragg_png = function(..., res = 500) {
  ragg::agg_png(..., res = res, units = "in")
}

knitr::opts_chunk$set(
  echo = TRUE, 
  class.output = "purple-output",
  dev = "ragg_png", 
  fig.align = "center"
)
```

<style>
  body {
    font-size: 16px;
    background-color: #fff9fa;
  }
  
  pre {
    border: 0px;
  }

  .sourceCode, code {
    font-family: 'Fira Mono';
  }
  
  code {
    background-color: #e5eeeb;
    color: #111;
  }
  
  pre.sourceCode {
    color: #111;
    font-weight: 500;
    background-color: #e5eeeb;
  }
  
  code span.fu {
    color: #0e4747;
    font-weight: 600;
  }
  
  code span.sc {
    color: #ae4d00;
    font-weight: 600;
  }
  
  code span.st {
    color: #ae4d00;
  }
  
  .purple-output{
    background-color: #012626;
    color: #eee;
  }
</style>


```{r warning=FALSE, message=FALSE}
# Dependencias 
# install.packages(c("tidyverse", "tidymodels","here", "janitor", reticulate", "keras"))

library(tidyverse)
library(tidymodels)
library(here)
startup_raw <- read_csv(file = here("Machine Learning Avanzado", "startup data.csv")) %>% 
  janitor::clean_names()
```


```{r echo=FALSE}
library(ggtext)
theme_set(theme_light(base_family = "Segoe UI", base_size = 16))
theme_update(
  plot.background = element_rect(fill = "#fff9fa", color = "transparent"),
  panel.background = element_rect(fill = "#eeeeee"),
  text = element_text(color = "#111111"),
  plot.title = element_markdown(hjust = .5),
  plot.subtitle = element_markdown(hjust = .5),
  strip.background = element_rect(fill = "#fff9fa"),
  strip.text = element_text(color = "#111111", face = "italic", size = "16")
)
```


El dataset `startup` registra información de empresas emergentes (startups) en Estados Unidos. En total, contiene 923 filas y 49 columnas. Cada fila representa a una startup y cada columna a una característica de ésta. Inicialmente, hay muchas columnas que pueden ser construidas a partir de éstas, causando así duplicidad de datos. Por tanto, el primer análisis consistirá en determinar cuáles columnas pueden ser excluidas del dataset sin perder información relevante.

Las variables dummy a descartar serán reconstruidas en el proceso de modelamiento, pero, por el momento, serán excluidas.


```{r}
startup <- startup_raw %>% 
  select(-c(unnamed_0, zip_code, id, unnamed_6, labels, closed_at, state_code_1, object_id),
         -starts_with("is_")) 

colnames(startup)
```


El dataset resultante tiene `ncol(startup)` columnas, `select(startup, where(is.character)) %>% ncol()` de las cuales son categóricas y las demás son numéricas. De las columnas categóricas, 3 corresponden a fechas, por lo que se procede a adaptarlas.


```{r}
startup <- startup %>% 
  mutate(across(
    matches("fo?und") & where(is.character),
    function(x) as.Date(x, format = "%m/%d/%Y")            
  )) 
```


A continuación, se exploran descriptivamente todas las variables, de acuerdo con su tipo de dato.
### Variables categóricas
- No hay *missing data* en las columnas categóricas. 
- Las empresas provienen de 35 estados; el 52,9% es de California (CA).
- En total son 221 ciudades. San Francisco tiene la primera mayoría (13,9%).
- Las startups se dedican a 35 rubros. Las dos grandes mayorías son software (16,6%) y web (15,6%).
- Existe desequilibrio entre las dos clases de la variable objetivo `status`: acquired (64,7%) y closed (35,3%).
- Las columnas `has_` *vc*, *angel*, *round_{a, b, c, d}* son variables dummy no mutuamente excluyentes.


### Variables fecha
- Las empresas fueron fundadas entre el 1 de enero de 1984 y el 16 de abril de 2013.
- Todas las empresas obtuvieron su primer financiamiento entre enero del 2000 y noviembre del 2013.
- Todas las empresas obtuvieron su último financiamiento entre enero del 2001 y noviembre del 2013.


### Variables numéricas

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(maps)
main_states <- map_data("state")
ggplot() +
  geom_polygon(data = main_states, aes(long, lat, group = group),
               fill = "#012626", color = "white", size = .1) +
  geom_point(data = startup, aes(longitude, latitude),
             inherit.aes = FALSE, fill = "gold3", size = 5, 
             alpha = .4, shape = 21, color = "gray30", stroke = 1) +
  coord_cartesian(xlim = c(-125, -69), ylim = c(25, 50)) +
  theme_void() +
  theme(plot.background = element_rect(fill = "#fff9fa"),
        panel.background = element_rect(fill = "#fff9fa"))
```


El mapa superior muestra las empresas registradas en USA. Solamente hay 4 empresas fuera de USA: Xelerated en el Mar Arábigo (Sureste de Omán), Moli, DiJiPOP y Tracelytics, las tres en el Océano Índico, al Este de Somalia. Por supuesto, las cuatro empresas se encuentran adscritas a ciudades estadounidenses, por lo que sus coordenadas son susceptibles a imputación. Las demás variables numéricas se describen a continuación:

- Los rangos de las variables de antigüedad de las empresas hasta su primer o último financiamiento e hito incluyen números negativos. Esto se interpreta como factible.
- La antigüedad de las empresas hasta su primer y último hito tiene missing data. Se detecta dependencia entre estas variables y el *status* de las empresas.
- Las correlaciones entre las variables numéricas se muestran en la matriz de correlación.


```{r echo=FALSE, warning=FALSE, message=FALSE}
corm <-
  startup %>%
  select(where(is.numeric), -starts_with("has_"), -milestones) %>%
  corrr::correlate(diagonal = 1) %>%
  corrr::shave(upper = FALSE)

corm <- corm %>%
  pivot_longer(
    cols = -term,
    names_to = "colname",
    values_to = "corr"
  ) %>%
  mutate(rowname = fct_inorder(term),
         colname = fct_inorder(colname))

corr_labs <- unique(corm$rowname)

ggplot(corm, aes(rowname, fct_rev(colname), fill = corr)) +
  geom_tile(size = 2) +
  geom_text(aes(
    label = format(round(corr, 2), nsmall = 2),
    color = abs(corr) < .4
  )) +
  scale_color_manual(values = c("white", "black"), 
                     guide = "none", na.value = "#fff9fa") +
  scale_fill_gradient2(
    low = "#e5eeeb", mid = "#e5eeeb", high = "#012626", na.value = "#fff9fa",
    limits = c(-1, 1) 
  ) +
  scale_x_discrete(labels = ifelse(str_count(corr_labs) > 10, 
                            str_remove_all(corr_labs, "[aeiou]"), 
                            as.character(corr_labs))) +
  labs(x = NULL, y = NULL, fill = expression(rho), 
       title = "Matriz de correlación") +
  # coord_fixed() +
  theme(panel.border = element_rect(color = NA, fill = NA),
        axis.text.y = element_text(margin = margin(0, 0, 0, 0)),
        axis.text.x = element_text(angle = 90),
        legend.background = element_rect(fill = "transparent"),
        legend.title = element_text(hjust = .1),
        legend.text.align = 1,
        legend.position = c(.75, .75),
        axis.ticks = element_blank())
        # plot.margin = unit(c(.3,.8,.3,.8), "cm"))
```

Se observa correlación moderada-alta entre la antigüedad de la startup en su primer y último financiamiento. Esta correlación indica que el tiempo que transcurre entre ambos eventos varía en forma lineal de empresa en empresa. A su vez, las variables mencionadas se correlacionan moderadamente con la antigüedad de la empresa en su primer/último hito.

Se estima que no hay correlaciones suficientemente altas entre los predictores, por ende, no se elimina ninguno del dataset. A continuación, se imputan los valores de latitud y longitud de las empresas ubicadas en el Mar Arábigo, con la media de sus vecinos más cercanos, utilizando las columnas `state_code`, `city` y `category_code`.

Respecto a las columnas `age_first_milestone_year` y `age_last_milestone_year`, ambas tienen 152 valores faltantes, en los mismos registros. Además, esto se condice con la columna `milestones`: la startup no registra milestones si y solo si adolesce de valores faltantes. Por simplicidad, las dos columnas `age_` serán excluidas del dataset.


```{r}
outliers <- startup %>% 
  slice_max(latitude, n = 4) %>% 
  pull(name)

startup_to_impute <- startup %>% 
  mutate(latitude = if_else(name %in% outliers, NA_real_, latitude),
         longitude = if_else(name %in% outliers, NA_real_, longitude))


impute_recipe <- recipe(~ ., data = startup_to_impute) %>% 
  update_role(name, new_role = "id") %>% 
  step_other(state_code, threshold = 10) %>% 
  step_other(city, threshold = 10) %>% 
  step_other(category_code, threshold = 10, other = "Other") %>% 
  step_impute_knn(longitude, impute_with = c("state_code", "city", "category_code")) %>% 
  step_impute_knn(latitude, impute_with = c("state_code", "city", "category_code"))

imputed_values <- impute_recipe %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  filter(name %in% outliers) %>% 
  select(lat = latitude, long = longitude, name)

startup_imputed <- startup_to_impute %>% 
  left_join(imputed_values, by = "name") %>% 
  mutate(latitude = if_else(is.na(latitude), lat, latitude),
         longitude = if_else(is.na(longitude), long, longitude)) %>% 
  select(-c(lat, long, age_first_milestone_year, age_last_milestone_year)) %>% 
  mutate(status = factor(status),
         id = row_number())
```


Finalmente, se particionan los datos en conjuntos de entrenamiento y prueba. A partir del conjunto de entrenamiento, se generan 10 conjuntos de validación utilizando* bootstrapping* estratificado.


```{r}
set.seed(123)
startup_split <- initial_split(startup_imputed, strata = status)
startup_train <- training(startup_split)
startup_test <- testing(startup_split)

set.seed(123)
startup_boots <- bootstraps(startup_train, times = 10, strata = status)
```


## Preprocesamiento de datos

Previo al entrenamiento de modelos con bootstrapping, se procede a realizar varias transformaciones sobre los datos:

- Se declara que el rol de las variables `name` y `id` es meramente identificador, es decir, no serán usadas para ajustar los modelos.
- Se agrupan las categorías con menos de 10 observaciones, de las variables `state_code`, `city` y `category_code`.
- Se crean variables dummy a partir de las categorías.
- Se extrae el año de las fechas.
- Se realiza una transformación logarítmica sobre las variables `funding_total_usd` y `avg_participants`, porque sus distribuciones son de gran varianza y sesgo.
- Se normalizan ($\mu=0$ y $\sigma=1$) los predictores numéricos.
- Se muestrea con reemplazo el nivel minoritario (closed) de la variable respuesta, hasta alcanzar un ratio de 1 entre las categorías.


```{r message=FALSE, warning=FALSE}
library(themis)

preprocessing <- recipe(status ~ ., data = startup_train) %>% 
  update_role(name, id, new_role = "id") %>% 
  step_other(state_code, threshold = 10) %>% 
  step_other(city, threshold = 10) %>% 
  step_other(category_code, threshold = 10, other = "Other") %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_date(founded_at, first_funding_at, last_funding_at,
            features = "year", keep_original_cols = FALSE) %>% 
  step_log(funding_total_usd, avg_participants) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_upsample(status)
```


## Entrenamiento de red neuronal 1 

Lo primero es crear una especificación de modelo de red neuronal MLP (multilayer perceptron), en este caso, de una sola capa. El hiperparámetro a optimizar es la cantidad de perceptrones en la capa oculta, denotado por el argumento `hidden_units = tune()`. Esta configuración, por defecto, especifica que se entrenará la red con una grilla de 10 valores, entre 1 y 10, para la cantidad de unidades ocultas. La función de activación por defecto para problemas de clasificación es *softmax*.


```{r}
neural_net_1 <- mlp(hidden_units = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("nnet")
neural_net_1
```


Luego, se incorporan tanto la especificación del modelo como las instrucciones de preprocesamiento en un objeto *workflow*.

```{r}
nnet_wf <- workflow() %>% 
  add_recipe(preprocessing) %>% 
  add_model(neural_net_1)
nnet_wf
```


```{r eval=FALSE}
library(doParallel)
ncores <- detectCores() - 1
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

nnet_grid_1 <- grid_regular(hidden_units(), levels = 10)

set.seed(123)
nnet_results_1 <- tune_grid(
  nnet_wf,
  resamples = startup_boots,
  grid = nnet_grid_1,
  control = control_grid(save_pred = TRUE, allow_par = TRUE)
)

stopImplicitCluster()
```


```{r echo=FALSE, eval=FALSE}
saveRDS(nnet_results_1, file = here("Machine Learning Avanzado", "nnet_results_1.rds"))
```


```{r echo=FALSE}
nnet_results_1 <- readRDS(file = here("Machine Learning Avanzado", "nnet_results_1.rds"))
```


## Entrenamiento de SVM

Para el modelo de SVM se utiliza el mismo preprocesador que para la red neuronal.


```{r}
svm_spec <- svm_rbf() %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")


svm_wf <- workflow() %>% 
  add_recipe(preprocessing) %>% 
  add_model(svm_spec)
svm_wf
```


```{r eval=FALSE}
registerDoParallel(cl)

svm_results <- fit_resamples(
  svm_wf, 
  resamples = startup_boots,
  control = control_resamples(save_pred = TRUE, allow_par = TRUE)
)

stopImplicitCluster()
```


```{r echo=FALSE, eval=FALSE}
saveRDS(svm_results, file = here("Machine Learning Avanzado", "svm_results.rds"))
```


```{r echo=FALSE}
svm_results <- readRDS(file = here("Machine Learning Avanzado", "svm_results.rds"))
```


```{r echo=FALSE}
svm_accuracy <- svm_results %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy")

svm_accuracy <- map_dfr(-1:11, function(x) svm_accuracy) %>% 
  mutate(hidden_units = -1:11)

nnet_results_1 %>% 
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>% 
  ggplot(aes(hidden_units, mean)) +
  geom_errorbar(aes(ymax = mean + std_err, ymin = mean - std_err), width = .3) +
  geom_point(size = 4, shape = 21, fill = "white", color = "#111111", stroke = 2) +
  geom_ribbon(data = svm_accuracy, 
              aes(ymax = mean + std_err, ymin = mean - std_err),
              alpha = .2) +
  geom_hline(data = svm_accuracy, aes(yintercept = mean), linetype = 2) +
  geom_curve(aes(x = 2, xend = 2.2, y = .6615, yend = .65), curvature = .2, size = .2) +
  geom_curve(aes(x = 2, xend = 2.8, y = .6865, yend = .65), curvature = -.3, size = .2) +
  annotate("text", x = 2.6, y = .64, size = 5,
           label= "Red Neuronal\nAccuracy ± error estándar") +
  annotate("text", x = 5.5, y = .702, size = 5,
           label= "Support Vector Machine\nAccuracy ± error estándar") +
  scale_y_continuous(labels = percent_format(), limits = c(.62, .72), expand = c(.01, .01)) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 11), 
                     labels = c(0, 2, 4, 6, 8, 10), breaks = c(0, 2, 4, 6, 8, 10)) + 
  labs(title = "Red neuronal vs SVM", y = "Accuracy", x = "# hidden units")
```


Evidentemente, la red neuronal no alcanza el accuracy del modelo SVM, de aproximadamente 70% de las clasificaciones correctas. Aún así, los resultados obtenidos por SVM no son alentadores. La matriz de confusión de SVM para todos los conjuntos de bootstrap expone que ambas clases son equitativamente difícíles para el modelo. Por otra parte, tras analizar la proporción de cada clase, es evidente que la tasa de `closed` verdaderos es menor que la de `acquired`. Por lo tanto, al modelo SVM predice con menor precisión que una startup haya cerrado, comparado con la red neuronal.


```{r}
svm_results %>% 
  collect_predictions() %>%
  count(id, .pred_class, status) %>% 
  group_by(.pred_class, status) %>% 
  summarize(pred_class = mean(n), .groups = "drop") %>% 
  pivot_wider(names_from = status, values_from = pred_class) 
```


## Entrenamiento de red neuronal 2 

El entrenamiento de la red neuronal profunda será llevado a cabo utilizando el paquete `keras`. Este framework necesita que los datos de entrenamiento y prueba ingresen al modelo en forma matricial.

```{r message=FALSE, warning=FALSE}
library(keras)

x_train <- preprocessing %>% 
  prep() %>% 
  juice(all_predictors(), composition = "matrix") 

y_train <- preprocessing %>% 
  prep() %>% 
  juice(all_outcomes()) %>% 
  pull() %>% 
  as.numeric() %>%
  {. - 1} %>% 
  to_categorical(num_classes = 2)

x_test <- preprocessing %>% 
  prep() %>% 
  bake(new_data = startup_test, all_predictors(), composition = "matrix")

y_test <- preprocessing %>% 
  prep() %>% 
  bake(new_data = startup_test, all_outcomes()) %>% 
  pull() %>% 
  as.numeric() %>%
  {. - 1} %>% 
  to_categorical(num_classes = 2)
```


La afinación de hiperparámetros la realiza la función `tuning_run()` del paquete `tfruns`. Esta función llama a un archivo de extensión `.R` que contiene al modelo, para variar sus parámetros y entrenar los modelos de manera iterativa. El script `keras_model.R` se muestra a continuación.


```{r eval=FALSE}
library(keras)

model_flags <- flags(
  flag_integer("layer_1", default = 10),
  flag_integer("layer_2", default = 10),
  flag_integer("layer_3", default = 10),
  flag_integer("layer_4", default = 10),
  flag_integer("layer_5", default = 10)
)

model_keras <- keras_model_sequential() %>% 
  layer_dense(units = model_flags$layer_1, activation = "relu", input_shape = 59) %>% 
  layer_dropout(rate = .4) %>% 
  layer_dense(units = model_flags$layer_2, activation = "relu") %>% 
  layer_dropout(rate = .3) %>% 
  layer_dense(units = model_flags$layer_3, activation = "relu") %>% 
  layer_dropout(rate = .2) %>% 
  layer_dense(units = model_flags$layer_4, activation = "relu") %>% 
  layer_dropout(rate = .1) %>% 
  layer_dense(units = model_flags$layer_5, activation = "relu") %>% 
  layer_dense(units = 2, activation = "softmax")

model_keras %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = .001),
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

history <- model_keras %>% fit(
  x_train, y_train,
  epochs = 20, 
  batch_size = 128,
  validation_split = .2
)

plot(history)

score <- model_keras %>% evaluate(
  x_test, y_test,
  verbose = 0
)
```

Solamente se entrena a una muestra del 1% de los modelos de 5 capas ocultas, de un total de 32800 arquitecturas. Esta celda de código es costosa de ejecutar, y se recomienda guardar los resultados una vez corrido el código, o descargar el archivo del [repositorio en github](https://github.com/DiabbZegpi/Data-Science-USS/tree/master/Machine%20Learning%20Avanzado).


```{r eval=FALSE}
# Optimización de hiperparámetros
tune_pars <- list(
  layer_1 = c(10, seq(20, 140, by = 20)),
  layer_2 = c(10, seq(20, 140, by = 20)),
  layer_3 = c(10, seq(20, 140, by = 20)),
  layer_4 = c(10, seq(20, 140, by = 20)),
  layer_5 = c(10, seq(20, 140, by = 20))
)

library(tfruns)

runs <- tuning_run(file = here("Machine Learning Avanzado", "keras_model.R"), 
                   runs_dir = here("Machine Learning Avanzado", "keras_runs"),
                   flags = tune_pars, 
                   sample = .01)
```


```{r echo=FALSE, eval=FALSE}
saveRDS(runs, file = here("Machine Learning Avanzado", "keras_runs.rds")) 
```


```{r echo=FALSE}
runs <- readRDS(file = here("Machine Learning Avanzado", "keras_runs.rds"))
```


## Análisis de los resultados

En la gráfica siguiente se aprecia que el incremento del número de neuronas cierra la brecha entre la exactitud de los conjuntos de entrenamiento y prueba. La mejor arquitectura tiene la forma (100, 60, 140, 10, 100), con un accuracy en el conjunto de validación del 73,7%.


```{r echo=FALSE}
runs <- runs %>% as_tibble()
runs %>% 
  rowwise() %>% 
  mutate(total_units = sum(c_across(flag_layer_1:flag_layer_5))) %>%
  ungroup() %>% 
  pivot_longer(cols = c(metric_accuracy, metric_val_accuracy)) %>% 
  ggplot(aes(total_units, value, fill = name)) +
  geom_point(alpha = .6, size = 3, shape = 21, color = "black") +
  geom_smooth(method = "loess", formula = y ~ x, color = "#111111") +
  scale_fill_manual(values = c("firebrick", "dodgerblue"),
                    labels = c("Entrenamiento", "Validación"),
                    name = NULL) +
  scale_y_continuous(labels = percent_format()) +
  labs(title = "Optimización de red neuronal profunda",
       x = "Total units",
       y = "Accuracy") +
  theme(legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent"))
```


Ahora se reentrena el modelo con la mejor combinación alcanzada.

```{r eval=FALSE}
library(keras)

model_keras <- keras_model_sequential() %>% 
  layer_dense(units = 100, activation = "relu", input_shape = 59) %>% 
  layer_dropout(rate = .4) %>% 
  layer_dense(units = 60, activation = "relu") %>% 
  layer_dropout(rate = .3) %>% 
  layer_dense(units = 140, activation = "relu") %>% 
  layer_dropout(rate = .2) %>% 
  layer_dense(units = 10, activation = "relu") %>% 
  layer_dropout(rate = .1) %>% 
  layer_dense(units = 100, activation = "relu") %>% 
  layer_dense(units = 2, activation = "softmax")

model_keras %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = .001),
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

model_keras %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = .001),
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

history <- model_keras %>% fit(
  x_train, y_train,
  epochs = 20, 
  batch_size = 128,
  validation_split = .2
)
```


```{r echo=FALSE, eval=FALSE}
save_model_tf(model_keras, filepath = here("Machine Learning Avanzado"))
```

```{r echo=FALSE}
model_keras <- load_model_tf(filepath = here("Machine Learning Avanzado"))
```


Para finalizar, se hacen predicciones en el conjunto de entrenamiento.

```{r warning=FALSE, message=FALSE}
preds <- model_keras %>% 
  predict(x_test) %>% 
  as_tibble(.name_repair = function(x) c("acquired", "closed")) %>% 
  bind_cols(preprocessing %>% 
              prep() %>% 
              bake(new_data = startup_test, all_outcomes())) %>% 
  mutate(pred = if_else(acquired > closed, "acquired", "closed"),
         pred = factor(pred))

preds %>% accuracy(truth = status, pred)
preds %>% conf_mat(truth = status, pred)
```

La red neuronal profunda supera al model SVM en accuracy. Además, debido al efecto de SMOTE, la matriz de confusión del SVM tiene más observaciones. Esto debió ser corregido durante la etapa de creación de la matriz `x_train`, pero no altera demasiado los resultados.

















